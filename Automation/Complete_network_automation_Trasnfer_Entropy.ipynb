{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Complete network automation - Trasnfer Entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPbuN1NI+qC6ozJh8uxkm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hegdenischay/NEURONNet/blob/main/Automation/Complete_network_automation_Trasnfer_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBtbl9iM-hha",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "size =   15#@param {type:\"integer\"}\n",
        "dimension = \"High\" #@param [\"High\", \"Low\"]\n",
        "epochs =  20 #@param {type:\"integer\"}\n",
        "A_plus = 0.01 #@param {type:\"number\"}\n",
        "A_minus = -0.0011 #@param {type: \"number\"}\n",
        "lateralDelay = 5 #@param {type: \"number\"}\n",
        "currentAmp = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_9d7LtSWFA0"
      },
      "source": [
        "# Backend Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHPcM6GtV5K_",
        "outputId": "c7637ee1-9ecd-44c2-ec49-71f78e93da87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting neuron\n",
            "  Downloading NEURON-8.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.0 MB 19.0 MB/s \n",
            "\u001b[?25hCollecting copent\n",
            "  Downloading copent-0.2.3-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from copent) (1.4.1)\n",
            "Installing collected packages: neuron, copent\n",
            "Successfully installed copent-0.2.3 neuron-8.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install pandas neuron copent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1452ffae"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "totalstart = time.time()\n",
        "import os\n",
        "if os.getcwd() != \"/content\":\n",
        "    # we are not in google colab, assume hebbian library is in current dir\n",
        "    from hebb import ORN, MCELL, GCELL\n",
        "else:\n",
        "    # download from git\n",
        "    import shutil, requests\n",
        "    url = 'https://cloud.operationtulip.com/s/t8nbn8Y582w94P3/download/git.zip'\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open('git.zip', 'wb') as out_file:\n",
        "        shutil.copyfileobj(response.raw, out_file)\n",
        "    del response\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('git.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63491b2a"
      },
      "outputs": [],
      "source": [
        "!nrnivmodl &>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYQ3O2_rte6s"
      },
      "outputs": [],
      "source": [
        "from neuron import h #.h is a HOC object instance & gui from neuron can also be imported \n",
        "from math import pi                  \n",
        "from neuron.units import ms,mV\n",
        "from hebb_test import ORN, MCELL, GCELL\n",
        "h.load_file('stdrun.hoc')                #Allows us to do a high level simulation\n",
        "pc = h.ParallelContext()\n",
        "import random  \n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "from bokeh.io import output_notebook\n",
        "import bokeh.plotting as plt2\n",
        "output_notebook()\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o2Z2RO4_o5B"
      },
      "outputs": [],
      "source": [
        "def stairstep(lo: float, hi: float, duration: int, max_duration: int = 200, noise: list = []) -> list:\n",
        "    # duration and max_duration are in ms\n",
        "    ans = [lo if i < duration else hi for i in range(0,max_duration)]\n",
        "    if len(noise) == 0:\n",
        "        return ans\n",
        "    else:\n",
        "        assert len(ans) == len(noise)\n",
        "        return [ans[i]+noise[i] for i in range(len(ans))]\n",
        "    # return [0+0.8*(i/duration) if i < duration else 0 for i in range(0,max_duration)]\n",
        "    # visualize input "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5odvQsJ7wzh_"
      },
      "outputs": [],
      "source": [
        "def netConMaker(source, dest, weights, delay, threshold, place='axon', type='exc'):\n",
        "    # print(source, dest)\n",
        "    if place == 'axon':\n",
        "        if type == 'exc':\n",
        "            netcon = h.NetCon(source.axon(0.5)._ref_v, dest.dendexcisyn, sec=source.axon)\n",
        "        elif type == 'inh':\n",
        "            netcon = h.NetCon(source.axon(0.5)._ref_v, dest.dendinhisyn, sec=source.axon)\n",
        "    elif place == 'soma':\n",
        "        if type == 'exc':\n",
        "            netcon = h.NetCon(source.soma(0.5)._ref_v, dest.dendexcisyn, sec=source.soma)\n",
        "        elif type == 'inh':\n",
        "            netcon = h.NetCon(source.soma(0.5)._ref_v, dest.dendinhisyn, sec=source.soma)\n",
        "    netcon.weight[0] = weights\n",
        "    netcon.delay = delay\n",
        "    netcon.threshold = threshold\n",
        "    return netcon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyVJT9eYTbt1"
      },
      "outputs": [],
      "source": [
        "def latConMaker(source, dest, weights, delay, threshold, type='exc'):\n",
        "    if type == 'exc':\n",
        "        netcon = h.NetCon(source.dend(0.5)._ref_v, dest.dendexcisyn, sec=source.dend)\n",
        "    else:\n",
        "        netcon = h.NetCon(source.dend(0.5)._ref_v, dest.dendinhisyn, sec=source.dend)\n",
        "    netcon.weight[0] = weights\n",
        "    netcon.delay = delay\n",
        "    netcon.threshold = threshold\n",
        "    return netcon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2RZsImWw1pX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "def plotCell(source, name, addendum=''):\n",
        "    recording_cell = source\n",
        "    cell_axon = h.Vector().record(recording_cell.axon(0.5)._ref_v)\n",
        "    cell_dend = h.Vector().record(recording_cell.dend(0.5)._ref_v)\n",
        "    t = h.Vector().record(h._ref_t)\n",
        "\n",
        "    h.finitialize(-70 * mV)\n",
        "    h.continuerun(300 * ms)\n",
        "\n",
        "    f = plt1.figure(figsize=(16,9))\n",
        "    ax = f.add_subplot(1,1,1)\n",
        "    ax.plot(t, list(cell_axon), label=name+' axon '+addendum)\n",
        "    ax.plot(t, list(cell_dend), label=name+' dendrite '+addendum)\n",
        "    ax.legend(loc=\"upper right\")\n",
        "    ax.set_xlabel('t (ms)')\n",
        "    ax.set_ylabel('v (mV)')\n",
        "    plt1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRU2J5WKw3Cm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def generate_L_weight_delta(first, second):\n",
        "    A_plus = 0.01\n",
        "    A_minus = -0.011\n",
        "    tau_pre =20*ms\n",
        "    tau_post =20*ms\n",
        "    delta_t = [(second[iter] - first[iter]) for iter in range(min(len(first),len(second)))]\n",
        "    delta_w_list = [A_plus*math.exp(-delta_t[iter]/tau_post) if delta_t[iter] >=0 else A_minus*math.exp(delta_t[iter]/tau_pre) for iter in range(min(len(first),len(second)))]  \n",
        "    delta_w = sum(delta_w_list)\n",
        "    return delta_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6OJ0E7jV4nG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGDhgsQEYa2X"
      },
      "outputs": [],
      "source": [
        "class AON:\n",
        "    def __init__(self,M,weights,delay1=1,delay2=2,delay3=3,delay4=4):\n",
        "        wMM=weights[0]\n",
        "        wMG=weights[1]\n",
        "        wGM=weights[2]\n",
        "        wGG=weights[3]\n",
        "        \n",
        "        self.M = M #Set no.\n",
        "        self.th = -70\n",
        "        self.maindelay = 5\n",
        "\n",
        "        self.M1=MCELL(1,self.M)\n",
        "        self.M2=MCELL(2,self.M)\n",
        "        self.G1=GCELL(3,self.M)\n",
        "        self.G2=GCELL(4,self.M)\n",
        "        self.cells = [self.M1, self.M2, self.G1, self.G2]\n",
        "        \n",
        "        self.nc0 = netConMaker(self.M1, self.M2, wMM, delay1+self.maindelay, self.th, 'soma') #Connecting E1 to E2\n",
        "        self.nc1 = netConMaker(self.M2, self.M1, wMM, delay2+self.maindelay, self.th, 'soma') #Connecting E2 to E1\n",
        "        self.nc2 = netConMaker(self.M2, self.G1, wMG, delay2+self.maindelay, self.th, 'soma') #Connecting E2 to I1\n",
        "        self.nc3 = netConMaker(self.G1, self.M2, wGM, delay3+self.maindelay, self.th, 'soma', type='inh') #Connecting I1 to E2\n",
        "        self.nc4 = netConMaker(self.G1, self.G2, wGG, delay3+self.maindelay, self.th, 'soma', type='inh') #Connecting I1 to I2\n",
        "        self.nc5 = netConMaker(self.G2, self.G1, wGG, delay4+self.maindelay, self.th, 'soma', type='inh') #Connecting I2 to I1\n",
        "        self.nc6 = netConMaker(self.G2, self.M1, wGM, delay4+self.maindelay, self.th, 'soma', type='inh') #Connecting I2 to E1\n",
        "        self.nc7 = netConMaker(self.M1, self.G2, wMG, delay1+self.maindelay, self.th, 'soma') #Connecting E1 to I2\n",
        "        self.nc8 = netConMaker(self.M1, self.G1, wMG, delay1+self.maindelay, self.th, 'soma') #Connecting E1 to I1\n",
        "        self.nc9 = netConMaker(self.G1, self.M1, wGM, delay3+self.maindelay, self.th, 'soma', type='inh') #Connecting I1 to E1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b8b7d01"
      },
      "outputs": [],
      "source": [
        "#Gives one 2 coupled neuron using freeman's topology in olfactory bulb\n",
        "class CONNECTING:\n",
        "\n",
        "    def __init__(self,M,weights,delay1=0,delay2=2,delay3=4,delay4=6):\n",
        "        wMM=weights[0]\n",
        "        wMG=weights[1]\n",
        "        wGM=weights[2]\n",
        "        wGG=weights[3]\n",
        "        \n",
        "        self.M = M #Set no.\n",
        "        self.th = -70\n",
        "        self.maindelay = 5\n",
        "        \n",
        "        #Making 2 neurons\n",
        "        self.orn = ORN(1,self.M)\n",
        "        self.P1=MCELL(2,self.M)\n",
        "        self.P2=MCELL(3,self.M)\n",
        "        self.cells = [self.orn, self.P1, self.P2]\n",
        "        \n",
        "        #Giving current pulse\n",
        "        self.stim = h.IClamp(self.orn.dend(0.5))  #P1.soma to P1.dend\n",
        "        # self.stimArr = [h.IClamp(self.orn.ciliumArr[i](0.5)) for i in range(len(self.orn.ciliumArr))]\n",
        "        self.stim.delay = 10\n",
        "        self.stim.dur = 100\n",
        "        self.stim.amp = currentAmp\n",
        "        \n",
        "        self.nc0 = netConMaker(self.orn, self.P1, wMM, self.maindelay+delay2, self.th, 'soma') #Connecting ORN to P1\n",
        "        self.nc1 = netConMaker(self.P1, self.P2, wMM, self.maindelay+delay3, self.th, 'soma') #Connecting P1 to P2\n",
        "        self.nc2 = netConMaker(self.P2, self.P1, wMM, self.maindelay+delay3, self.th, 'soma') #Connecting P2 to P1\n",
        "    \n",
        "        # 4-coupled\n",
        "\n",
        "        self.M1=MCELL(1,self.M)\n",
        "        self.M2=MCELL(2,self.M)\n",
        "        self.G1=GCELL(3,self.M)\n",
        "        self.G2=GCELL(4,self.M)\n",
        "        self.cells.extend([self.M1, self.M2, self.G1, self.G2])\n",
        "\n",
        "        self.nc14 = netConMaker(self.orn, self.M1, wMM, delay1+self.maindelay, self.th, 'soma')\n",
        "        self.nc3 = netConMaker(self.P1, self.M1, wMM, delay1+self.maindelay, self.th, 'soma')\n",
        "        self.nc4 = netConMaker(self.M1, self.M2, wMM, delay2+self.maindelay, self.th, 'soma') #Connecting M1 to M2\n",
        "        self.nc5 = netConMaker(self.M2, self.M1, wMM, delay2+self.maindelay, self.th, 'soma') #Connecting M2 to M1\n",
        "        self.nc6 = netConMaker(self.M2, self.G1, wMG, delay2+self.maindelay, self.th, 'soma') #Connecting M2 to G1\n",
        "        self.nc7 = netConMaker(self.G1, self.M2, wGM, delay2+self.maindelay, self.th, 'soma', type='inh') #Connecting G1 to M2\n",
        "        self.nc8 = netConMaker(self.G1, self.G2, wGG, delay3+self.maindelay, self.th, 'soma', type='inh') #Connecting G1 to G2\n",
        "        self.nc9 = netConMaker(self.G2, self.G1, wGG, delay3+self.maindelay, self.th, 'soma', type='inh') #Connecting G2 to G1\n",
        "        self.nc10 = netConMaker(self.G2, self.M1, wGM, delay4+self.maindelay, self.th, 'soma', type='inh') #Connecting G2 to M1\n",
        "        self.nc11 = netConMaker(self.M1, self.G2, wMG, delay4+self.maindelay, self.th, 'soma') #Connecting M1 to G2\n",
        "        self.nc12 = netConMaker(self.M1, self.G1, wMG, delay1+self.maindelay, self.th, 'soma') #Connecting M1 to G1\n",
        "        self.nc13 = netConMaker(self.G1, self.M1, wGM, delay1+self.maindelay, self.th, 'soma', type='inh') #Connecting G1 to M1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3d8b0fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LATERAL:\n",
        "    \"\"\"Laterally coupling N-2 coupled neurons\n",
        "    \"\"\"\n",
        "    def __init__(self,N,weights,delay1=0,delay2=2,delay3=4,delay4=6):\n",
        "        wMML=weights[1]\n",
        "        wGGL=weights[5]\n",
        "        \n",
        "        self.N = N\n",
        "        self.th = -70\n",
        "        self.maindelay = lateralDelay\n",
        "        self.sets = [CONNECTING(i, weights) for i in range(N)]\n",
        "            \n",
        "        self.twoCupArr = []\n",
        "        for r in range(0,N-1):\n",
        "          #Connecting P1[0] to P1[1]\n",
        "          # make a list for easier access\n",
        "          netConList = []\n",
        "          netConList.append(latConMaker(self.sets[r].P1, self.sets[r+1].P1,wMML,self.maindelay+delay2,self.th)) #P1[0] to P1[1]\n",
        "          netConList.append(latConMaker(self.sets[r+1].P1, self.sets[r].P1, wMML, self.maindelay+delay2,self.th))\n",
        "\n",
        "          self.twoCupArr.append(netConList)\n",
        "        self.fourCupArr = []\n",
        "        for r in range(1,N-1):\n",
        "            for j in range(N-r):\n",
        "                netConList = []\n",
        "                netConList.append(latConMaker(self.sets[j].M1, self.sets[j+r].M1, wMML, self.maindelay+delay3, self.th)) #Connect M1[0] to M1[1]\n",
        "                netConList.append(latConMaker(self.sets[j+r].M1, self.sets[j].M1, wMML, self.maindelay+delay3, self.th)) #Connect M1[1] to M1[0]\n",
        "                netConList.append(latConMaker(self.sets[j].G1, self.sets[j+r].G1, wGGL, self.maindelay+delay2, self.th, 'inh')) #Connecting G1[0] to G1[1]\n",
        "                netConList.append(latConMaker(self.sets[j+r].G1, self.sets[j].G1, wGGL, self.maindelay+delay2, self.th, 'inh')) #Connecting G1[1] to G1[0]\n",
        "                self.fourCupArr.append(netConList)\n",
        "\n",
        "        # add AON and PC\n",
        "        self.AON = AON(5,weights)\n",
        "        self.PC = AON(6,weights)\n",
        "        self.DPC = GCELL(7,self.N)\n",
        "        # add noise to AON's M1\n",
        "        self.stim = h.IClamp(self.AON.M1.dend(0.5))  #P1.soma to P1.dend\n",
        "        self.noise = np.random.normal(0, 0.1, 100)\n",
        "        self.input = h.Vector(self.noise)\n",
        "        self.tv = h.Vector([i for i in range(100)])\n",
        "        self.input.play(self.stim._ref_amp, self.tv, True)\n",
        "        self.otherNetCons = []\n",
        "        self.otherNetCons.append(netConMaker(self.sets[0].M1, self.AON.M1, wMML, self.maindelay+delay2, self.th, 'soma')) # 4-coupled M1 -> AON's E1\n",
        "        for i in self.sets:\n",
        "            for j in self.sets:\n",
        "                if i != j:\n",
        "                    self.otherNetCons.append(netConMaker(i.P1, j.P1, wMML, self.maindelay+delay2, self.th, 'soma')) # all-to-all in P1\n",
        "                    self.otherNetCons.append(netConMaker(i.M1, j.M1, wMML, self.maindelay+delay3, self.th, 'soma')) # all-to-all in M1\n",
        "                    self.otherNetCons.append(netConMaker(i.G1, j.G1, wGGL, self.maindelay+delay2, self.th, 'soma')) # all-to-all in M1\n",
        "\n",
        "        for i in self.sets:\n",
        "            self.otherNetCons.append(netConMaker(self.AON.M1, i.P2, wMML, self.maindelay+delay2, self.th, 'soma')) # AON E1 -> 2-coupled P2\n",
        "        if dimension == \"High\":\n",
        "            self.otherNetCons.append(netConMaker(self.PC.M1, self.AON.G1, wMML, self.maindelay+delay3+10, self.th, 'soma')) # PC's A1 to AON's I1\n",
        "        self.otherNetCons.append(netConMaker(self.sets[0].M1, self.PC.M1, wMML, self.maindelay+delay2, self.th, 'soma')) # M1 to A1\n",
        "        for i in self.sets:\n",
        "            self.otherNetCons.append(netConMaker(self.AON.M1, i.G1, wMML, self.maindelay+delay2, self.th, 'soma')) # AON's E1 -> 4-coupled G1\n",
        "        \n",
        "        self.otherNetCons.append(netConMaker(self.PC.G1, self.DPC, wMML/100000, self.maindelay+delay3, self.th, 'soma', 'inh')) # PC's B1 -> DPC\n",
        "        self.otherNetCons.append(netConMaker(self.DPC, self.PC.G1, wMML/100000, self.maindelay+delay3, self.th, 'soma')) # DPC -> PC's B1\n",
        "        if dimension == \"High\":\n",
        "            for i in self.sets:\n",
        "                self.otherNetCons.append(netConMaker(self.DPC, i.G1, wMML/100000, self.maindelay+delay3+10, self.th))# DPC to G1 \n",
        "        self.otherNetCons.append(netConMaker(self.sets[-1].G1, self.DPC, wMML, self.maindelay+4, self.th, 'soma'))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "665f6425"
      },
      "outputs": [],
      "source": [
        "#Give N value to get N no. of 2 coupled neurons laterally connected\n",
        "#given_input_from_user = int(input('Enter the number of 2 coupled sets required: '))\n",
        "import numpy as np\n",
        "# size = 5\n",
        "low, high = 1, 5\n",
        "# all_weights = np.random.uniform(low, high,2*given_input_from_user-1)\n",
        "weights = [4.65058555, 2.92840154, 3.68134116, 1.30487304, 1.07771946, 3.15136447, 1.04247865, 2.29878494, 2.01195344]\n",
        "# all_weights = [0.95336892, 0.87793015, 0.71060386, 0.88298338, 0.84561174, 0.5006465, 0.60983793, 0.88890724, 0.85189888]\n",
        "L1=LATERAL(size,weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xfmf2k-NDN29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ba3569-84f7-4a3c-a074-4be5c7557278"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='20'\n",
              "            max='20',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            20\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time it took to learn 46.63106608390808 seconds\n"
          ]
        }
      ],
      "source": [
        "#@title Learning Mechanism { vertical-output: true }\n",
        "import sys, time\n",
        "tick = time.time()\n",
        "weight_data = []\n",
        "\n",
        "out = display(progress(0, epochs), display_id=True)\n",
        "\n",
        "for l in range(epochs):\n",
        "    #print(\"iteration no is\",l)\n",
        "    out.update(progress(l, epochs))    \n",
        "    # print(\"\\nmutual learning\")\n",
        "    P1_axon_data = []\n",
        "    P1_dend_data = []\n",
        "    P2_axon_data = []\n",
        "    P2_dend_data = []\n",
        "    M1_axon_data = []\n",
        "    M1_dend_data = []\n",
        "    M2_axon_data = []\n",
        "    M2_dend_data = []\n",
        "    G1_axon_data = []\n",
        "    G1_dend_data = []\n",
        "    G2_axon_data = []\n",
        "    G2_dend_data = []\n",
        "    P1_spike_times = []\n",
        "    P2_spike_times = []\n",
        "    M1_spike_times = []\n",
        "    M2_spike_times = []\n",
        "    G1_spike_times = []\n",
        "    G2_spike_times = []\n",
        "\n",
        "    h.finitialize(-70* mV)\n",
        "    h.continuerun(300*ms)\n",
        "    t = h.Vector().record(h._ref_t)\n",
        "    \n",
        "    for i in range(len(L1.sets)):\n",
        "        P1_axon_data.append(list(h.Vector().record(L1.sets[i].P1.axon(0.5)._ref_v)))\n",
        "        P1_dend_data.append(list(h.Vector().record(L1.sets[i].P1.dend(0.5)._ref_v)))\n",
        "        P1_spike_times.append(list(L1.sets[i].P1.spike_times))\n",
        "        P2_axon_data.append(list(h.Vector().record(L1.sets[i].P2.axon(0.5)._ref_v)))\n",
        "        P2_spike_times.append(list(L1.sets[i].P2.spike_times))\n",
        "        P2_dend_data.append(list(h.Vector().record(L1.sets[i].P2.dend(0.5)._ref_v)))\n",
        "        M1_axon_data.append(list(h.Vector().record(L1.sets[i].M1.axon(0.5)._ref_v)))\n",
        "        M1_dend_data.append(list(h.Vector().record(L1.sets[i].M1.dend(0.5)._ref_v)))\n",
        "        M1_spike_times.append(list(L1.sets[i].M1.spike_times))\n",
        "        M2_axon_data.append(list(h.Vector().record(L1.sets[i].M2.axon(0.5)._ref_v)))\n",
        "        M2_dend_data.append(list(h.Vector().record(L1.sets[i].M2.dend(0.5)._ref_v)))\n",
        "        M2_spike_times.append(list(L1.sets[i].M2.spike_times))\n",
        "        G1_axon_data.append(list(h.Vector().record(L1.sets[i].G1.axon(0.5)._ref_v)))\n",
        "        G1_dend_data.append(list(h.Vector().record(L1.sets[i].G1.dend(0.5)._ref_v)))\n",
        "        G1_spike_times.append(list(L1.sets[i].G1.spike_times))\n",
        "        G2_axon_data.append(list(h.Vector().record(L1.sets[i].G2.axon(0.5)._ref_v)))\n",
        "        G2_dend_data.append(list(h.Vector().record(L1.sets[i].G2.dend(0.5)._ref_v)))\n",
        "        G2_spike_times.append(list(L1.sets[i].G2.spike_times))\n",
        "\n",
        "   \n",
        "    # h.finitialize(-70 * mV)\n",
        "\n",
        "    for i in range(len(L1.sets)):\n",
        "        # 2-coupled STDP-based learning\n",
        "        L1.sets[i].nc1.weight[0] += generate_L_weight_delta(P1_spike_times[i], P2_spike_times[i])\n",
        "        # logging\n",
        "        # print(\"P1_axon\", P1_spike_times[i], P2_spike_times[i])\n",
        "        L1.sets[i].nc2.weight[0] += generate_L_weight_delta(P2_spike_times[i], P1_spike_times[i])\n",
        "        L1.sets[i].nc3.weight[0] += generate_L_weight_delta(P1_spike_times[i], M1_spike_times[i])\n",
        "        # 4-coupled STDP-based learning\n",
        "        L1.sets[i].nc4.weight[0] += generate_L_weight_delta(M1_spike_times[i], M2_spike_times[i])\n",
        "        L1.sets[i].nc5.weight[0] += generate_L_weight_delta(M2_spike_times[i], M1_spike_times[i])\n",
        "        L1.sets[i].nc6.weight[0] += generate_L_weight_delta(M2_spike_times[i], G1_spike_times[i])\n",
        "        L1.sets[i].nc7.weight[0] += generate_L_weight_delta(G1_spike_times[i], M2_spike_times[i])\n",
        "        L1.sets[i].nc8.weight[0] += generate_L_weight_delta(G1_spike_times[i], G2_spike_times[i])\n",
        "        L1.sets[i].nc9.weight[0] += generate_L_weight_delta(G2_spike_times[i], G1_spike_times[i])\n",
        "        L1.sets[i].nc10.weight[0] += generate_L_weight_delta(G2_spike_times[i], M1_spike_times[i])\n",
        "        L1.sets[i].nc11.weight[0] += generate_L_weight_delta(M1_spike_times[i], G2_spike_times[i])\n",
        "        L1.sets[i].nc12.weight[0] += generate_L_weight_delta(M1_spike_times[i], G1_spike_times[i])\n",
        "        L1.sets[i].nc13.weight[0] += generate_L_weight_delta(G1_spike_times[i], M1_spike_times[i])\n",
        "        # L1.sets[i].DPC.weight[0] += generate_L_weight_delta(G1_spike_times[i], M1_spike_times[i])\n",
        "        \n",
        "    # do a proper logging function\n",
        "    weight_data.append(L1.sets[0].nc1.weight[0])\n",
        "\n",
        "\n",
        "    #Lateral Learning\n",
        "    for i in range(len(L1.sets)-1):\n",
        "        # 2-coupled part\n",
        "        L1.twoCupArr[i][0].weight[0] += generate_L_weight_delta(P1_spike_times[i], P1_spike_times[i+1])\n",
        "        L1.twoCupArr[i][1].weight[0] += generate_L_weight_delta(P1_spike_times[i+1], P1_spike_times[i])\n",
        "        # 4-coupled part\n",
        "        L1.fourCupArr[i][0].weight[0] += generate_L_weight_delta(M1_spike_times[i], M1_spike_times[i+1])\n",
        "        L1.fourCupArr[i][1].weight[0] += generate_L_weight_delta(M1_spike_times[i+1], M1_spike_times[i])\n",
        "        L1.fourCupArr[i][2].weight[0] += generate_L_weight_delta(G1_spike_times[i], G1_spike_times[i+1])\n",
        "        L1.fourCupArr[i][3].weight[0] += generate_L_weight_delta(G1_spike_times[i+1], G1_spike_times[i])\n",
        "    # plotCell(L1.sets[-1].M1, 'M1')\n",
        "\n",
        "\n",
        "out.update(progress(epochs, epochs))    \n",
        "tock = time.time()\n",
        "print(\"The time it took to learn\", tock-tick, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output"
      ],
      "metadata": {
        "id": "UXSzckrbmxcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copent import transent\n",
        "import numpy as np\n",
        "resM1 = h.Vector().record(L1.sets[0].M1.axon(0.5)._ref_v)\n",
        "resM2 = h.Vector().record(L1.sets[0].M2.axon(0.5)._ref_v)\n",
        "M1M2Arr = []\n",
        "M2M1Arr = []\n",
        "G1G2Arr = []\n",
        "G2G1Arr = []\n",
        "P1P2Arr = []\n",
        "P2P1Arr = []\n",
        "M1Arr = []\n",
        "M2Arr = []\n",
        "G1Arr = []\n",
        "G2Arr = []\n",
        "P1Arr = []\n",
        "P2Arr = []\n",
        "for i in L1.sets:\n",
        "    M1Arr.append(h.Vector().record(i.M1.axon(0.5)._ref_v))\n",
        "    M2Arr.append(h.Vector().record(i.M2.axon(0.5)._ref_v))\n",
        "    G1Arr.append(h.Vector().record(i.G1.axon(0.5)._ref_v))\n",
        "    G2Arr.append(h.Vector().record(i.G2.axon(0.5)._ref_v))\n",
        "    P1Arr.append(h.Vector().record(i.P1.axon(0.5)._ref_v))\n",
        "    P2Arr.append(h.Vector().record(i.P2.axon(0.5)._ref_v))\n",
        "h.finitialize(-70*mV)\n",
        "h.continuerun(300*ms)\n",
        "for i in L1.sets:\n",
        "    for j in range(len(M1Arr)):\n",
        "        M1M2Arr.append(transent(list(M1Arr[j]), list(M2Arr[j]), 2))\n",
        "        M2M1Arr.append(transent(list(M2Arr[j]), list(M1Arr[j]), 2))\n",
        "        G1G2Arr.append(transent(list(G1Arr[j]), list(G2Arr[j]), 2))\n",
        "        G2G1Arr.append(transent(list(G2Arr[j]), list(G1Arr[j]), 2))\n",
        "        P1P2Arr.append(transent(list(P1Arr[j]), list(P2Arr[j]), 2))\n",
        "        P2P1Arr.append(transent(list(P2Arr[j]), list(P1Arr[j]), 2))"
      ],
      "metadata": {
        "id": "2t7haKnDm2-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}